---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This is the md5 of the environment file
# this will ensure consistency when deploying.
environment_version: 21de6ac181f7f9d5d950709928ccded8

# User defined CIDR used for containers
# Global cidr/s used for everything.
cidr_networks:
  # Cidr used in the Management network
  container: 172.29.236.0/22
  # Cidr used in the Service network
  snet: 172.29.248.0/22
  # Cidr used in the VM network
  tunnel: 172.29.240.0/22
  # Cidr used in the Storage network
  storage: 172.29.244.0/22

# User defined list of consumed IP addresses that may intersect 
# with the provided CIDR.
used_ips:
  - 172.29.236.1,172.29.236.50
  - 172.29.244.1,172.29.244.50

# As a user you can define anything that you may wish to "globally"
# override from within the rpc_deploy configuration file. Anything 
# specified here will take precedence over anything else any where.
global_overrides:
  # Internal Management vip address
  internal_lb_vip_address: 172.29.236.1
  # External DMZ VIP address
  external_lb_vip_address: 192.168.1.1
  # Bridged interface to use with tunnel type networks
  tunnel_bridge: "br-vxlan"
  # Bridged interface to build containers with
  management_bridge: "br-mgmt"
  # Define your Add on container networks.
  #  group_binds: bind a provided network to a particular group
  #  container_bridge: instructs inventory where a bridge is plugged
  #                    into on the host side of a veth pair
  #  container_interface: interface name within a container
  #  ip_from_q: name of a cidr to pull an IP address from
  #  type: Networks must have a type. types are: ["raw", "vxlan", "flat", "vlan"]
  #  range: Optional value used in "vxlan" and "vlan" type networks
  #  net_name: Optional value used in mapping network names used in neutron ml2
  # You must have a management network.
  provider_networks:
    - network:
        group_binds:
          - all_containers
          - hosts
        type: "raw"
        container_bridge: "br-mgmt"
        container_interface: "eth1"
        ip_from_q: "container"
    - network:
        group_binds:
          - glance_api
          - cinder_api
          - cinder_volume
          - nova_compute
        type: "raw"
        container_bridge: "br-storage"
        container_interface: "eth2"
        ip_from_q: "storage"
    - network:
        group_binds:
          - glance_api
          - nova_compute
          - neutron_linuxbridge_agent
        type: "raw"
        container_bridge: "br-snet"
        container_interface: "eth3"
        ip_from_q: "snet"
    - network:
        group_binds:
          - neutron_linuxbridge_agent
        container_bridge: "br-vxlan"
        container_interface: "eth10"
        ip_from_q: "tunnel"
        type: "vxlan"
        range: "1:1000"
        net_name: "vxlan"
    - network:
        group_binds:
          - neutron_linuxbridge_agent
        container_bridge: "br-vlan"
        container_interface: "eth11"
        type: "flat"
        net_name: "vlan"
    - network:
        group_binds:
          - neutron_linuxbridge_agent
        container_bridge: "br-vlan"
        container_interface: "eth11"
        type: "vlan"
        range: "1:1"
        net_name: "vlan"
  #
  # Setup swift group variables when using swift
  # part power is required under swift.
  # For account/container speciying min_part_hours and repl_number is all thats required
  # Alternatively defaults will be used (repl_number of 3, and min_part_hours of 1).
  # For storage policies, a name and unique index is required as well as repl_number and
  # min_part_hours which will be set to a default value if not specified.
  # There MUST be a storage policy with index 0 configured which will be the default for legacy containers (created pre-storage policies).
  # You can set one policy to be "default: yes" this will be the default storage policy for non-legacy containers that are created.
  # The index value must be unique.
  # Storage policies can be set to "deprecated: yes" which will mean they are not used
  # swift:
  #   part_power: 8
  #   account:
  #     repl_number: 3
  #     min_part_hours: 1
  #   container:
  #     repl_number: 3
  #   storage_policies:
  #    - policy:
  #        name: gold
  #        index: 0
  #        repl_number: 3
  #        default: yes
  #    - policy:
  #        name: silver
  #        index: 1
  #        repl_number: 2
  #        deprecated: yes
  #
  # Name of load balancer
  lb_name: lb_name_in_core

# User defined Infrastructure Hosts, this should be a required group
infra_hosts:
  infra1:
    ip: 172.29.236.100
  infra2:
    ip: 172.29.236.101
  infra3:
    ip: 172.29.236.102

# User defined Compute Hosts, this should be a required group
compute_hosts:
  compute1:
    ip: 172.29.236.103

# User defined Storage Hosts, this should be a required group
storage_hosts:
  cinder1:
    ip: 172.29.236.104
    # "container_vars" can be set outside of all other options as 
    # host specific optional variables.
    container_vars:
      # In this example we are defining what cinder volumes are 
      # on a given host.
      cinder_backends:
        # if the "limit_container_types" argument is set, within 
        # the top level key of the provided option the inventory
        # process will perform a string match on the container name with
        # the value found within the "limit_container_types" argument.
        # If any part of the string found within the container 
        # name the options are appended as host_vars inside of inventory.
        limit_container_types: cinder_volume
        lvm:
          volume_group: cinder-volumes
          volume_driver: cinder.volume.drivers.lvm.LVMISCSIDriver
          volume_backend_name: LVM_iSCSI

# User defined Logging Hosts, this should be a required group
log_hosts:
  logger1:
    ip: 172.29.236.107

# User defined Networking Hosts, this should be a required group
network_hosts:
  network1:
    ip: 172.29.236.108

# User defined Object Storage Hosts - this is not a required group
# Under swift_vars you can specify the host specific swift_vars.
# region - the swift region, this isn't required.
# zone - the swift zone, this isn't required either, will default to 0
# mount_point - where the drives are mounted on the server
# drives - A list of drives in the server (Must have a name as a minimum)
# Above 4 vars are "host specific"
# weight: a disks weight (defaults to 100 if not specified)
# repl_ip: IP specific for object replication (not required)
# repl_port: Port specific for object replication (not required)
# groups: A list of groups to add the drive to. A group is either a storage policy or the account or container servers. (If not specified defaults to all groups, so container/account/all storage policies).
# The above 4 can be specified on a per host or per drive basis
# Or both, in which case "per drive" will take precedence for the specific drive.
# ip can be specified in swift_vars to override the hosts ip
# or per drive to override all for that specific drive.
# swift_hosts: 
#   object_storage1:
#    ip: 172.29.236.108
#    container_vars:
#      swift_vars:
#        region: 0
#        zone: 0
#        groups:
#          - silver
#          - account
#        mount_point: /srv/node
#        drives:
#          - name: sdb
#            ip: 172.10.100.100
#            repl_ip: 10.10.0.1
#            repl_port: 54321
#            groups:
#              - gold
#              - account
#              - container
#          - name: sdc
#            weight: 150
#          - name: sdd
#          - name: sde
#
#   object_storage2:
#    ip: 172.29.236.109
#    container_vars:
#      swift_vars:
#        region: 0
#        zone: 1
#        mount_point: /srv/node
#        drives:
#          - name: sdb
#          - name: sdc
